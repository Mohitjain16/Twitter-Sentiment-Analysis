# -*- coding: utf-8 -*-
"""Twitter Sentiment Analysis_2022pcp5434.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15PSeMHzg4EUiFCiQOoIQDGqJ98UyfaQy
"""

import pandas as pd
import numpy as np
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import string
from nltk import pos_tag
import warnings
warnings.filterwarnings("ignore")
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn import tree

df_train = pd.read_csv('train.csv')

df_train = df_train[['text', 'airline_sentiment']]
training_data = df_train.values

#Spliiting the Tweet text into words using NLTK
tweets_train = []
for i in range(len(training_data)):
    tweets_train.append([word_tokenize(training_data[i][0]), training_data[i][1]])

#taking only two colum as text and airline _sentimant( mean text is positive ,negative or neutral)
df_train = df_train[['text', 'airline_sentiment']]
# here df_train.values convert the list or data  in 2d array (convert in 2 D array)
training_data = df_train.values

tweets_train[0]

#Cleaning the Words using WordNetLemmatizer available in NLTK
stops = set(stopwords.words('english'))
punctuations = list(string.punctuation)
stops.update(punctuations)

from nltk.corpus import wordnet
def get_simple_pos(tag):

    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

lemmatizer = WordNetLemmatizer()
def clean_tweets(words):
    output_words = []
    for w in words:
        if w.isalpha():
            if w.lower() not in stops:
                pos = pos_tag([w])
                clean_word = lemmatizer.lemmatize(w, pos = get_simple_pos(pos[0][1]))
                output_words.append(clean_word.lower())
    return output_words

for i in range(len(tweets_train)):
    tweets_train[i] = (clean_tweets(tweets_train[i][0]), tweets_train[i][1])

y_train = []
tweets = []
for tweet, sentiment in tweets_train:
    tweets.append(" ".join(tweet))
    y_train.append(sentiment)

#Using Count Vectorizer to get the X Train
count_vec = CountVectorizer(max_features=2000)
x_train_features = count_vec.fit_transform(tweets)

#Prepaing Testing Data
df_test = pd.read_csv('test.csv')
testing_data = np.array(df_test['text'])

tweets_test = []
for t in testing_data:
    t = clean_tweets(word_tokenize(t))
    tweets_test.append(" ".join(t))

x_test_features = count_vec.transform(tweets_test)

"""# Classification - MultiNomialNB"""

mnv = MultinomialNB(alpha = 1)
mnv.fit(x_train_features, y_train)

y_pred_mnv = mnv.predict(x_test_features)
y_pred_mnv

print(mnv.score(x_test_features,y_pred_mnv))
print(mnv.score(x_train_features,y_train))

#Store predictions
df = pd.DataFrame(y_pred_mnv)
df.to_csv('predictions_dt.csv', index = False, header = False)